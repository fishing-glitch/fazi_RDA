---
title: "Cocaine Trees - Predicting Prices with Random Forests"
output: html_notebook
---

### Introduction

The following analysis utilises data comprising of cocaine prices available on an online hidden marketplace called Dream Market, available on the deep web. This data was freely obtained at https://www.kaggle.com/everling/cocaine-listings, with further details about the data available in the description. As the title might suggest, the goal of this analysis is attempting to accurately predict the cocaine prices based on the variables provided, utlising the Random Forests (RF) method. The RF model combines several weak learners (individual decision trees) to form a single strong ensemble model. The RF method is chosen simply because it is flexible enough to incorporate various kinds of data, and has the ability to provide a low level of bias along with a moderate level of variance, a balance that is often hard to achieve in ML models. The analysis is kept as simple as possible, without any significant complications.

Loading the data and required libraries:

```{r message=FALSE, warning=FALSE}
#Libraries
library(tidyverse)
library(rsample)
library(recipes)
library(caret)
library(plotly)

#Loading Data
cocaine <- read.csv("dream-market-cocaine-listings.csv")
```

### Visualizing and Exploring

At first sight, the data may seem somewhat complicated due to the large number of variables (64 in total). However, a vast majority of these are only catergorical, highlighting whether the product was shipped to or from a certain region. Other important variables include grams sold, the cost per gram, the quality of the product, escrow (whether a third party was involved for transaction security), successful transactions by the vendor and the customer ratings given. As cost per gram is the target variable in this analysis, exploration can begin from here. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
# distribution of cost per gram...
ggplot(cocaine, aes(cost_per_gram)) +
  geom_histogram(aes(cost_per_gram, ..density..),fill = "lightblue",
                 color = "black", bins = 40) +
  geom_density(size = 1.25) +
  theme_minimal()
```

The histogram of the cost per gram variable shows a right skewed distribution. The prices are listed in Bitcoin currency. Majority of the prices fall below 0.05 BTC, which implies that there is some homogeneity involved in how closely the prices listed by various vendors are similar to each other. The cocaine market is apparently more competitive than one would think.  

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#avg ratings...
ggplot(cocaine, aes(rating)) +
  geom_histogram(bins = 20, fill = "lightblue", color = "black") +
  theme_minimal()
```

The ratings of cocaine being sold is quite high, with no product being rated less than 4.4/5. Majority of the ratings are between 4.8 and 5, implying strong customer satisfaction from the product, and also a left skewed distribution. The 4.4 rating can also be considered an outlier if compared with how the other ratings are distributed. Perhaps there were problems with delivery, or the batch wasnt as high quality as usual, or some other factors were involved. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#scatter plot for quality vs cost_per gram (interactive)
ggplotly(ggplot(cocaine, aes(quality, cost_per_gram)) +
  geom_jitter(alpha = 0.5) +
  theme_minimal())
```

While the initial impression may be that costs and quality should be directly proportional with each other (a rise in quality increases price), the plot above shows that this is not strictly true. A clear linear relationship cannot be observed, and for a certain measure of quality, there are numerous prices being quoted. This can be seen especially for products with quality quoted at 90 having numerous prices. It can be concluded that there are probably some other factors strongly involved apart from quality that are determining the cost per gram of the product. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#interaction between price and ships_from including quality...
cocaine %>%
  group_by(ships_from) %>%
  summarise(avg_quality = mean(quality), 
            avg_price = mean(cost_per_gram)) %>%
  ggplot(aes(reorder(ships_from, avg_price), avg_price)) +
  geom_col(aes(fill = avg_quality)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

To understand the relationship between location and prices, the data is gathered and reordered according to where the product is shipped from and where it is shipped to. The former is plotted above, providing some interesting insights. The highest quality product is shipped from China, which also surprisingly involves the lowest average price. This is opposite in Australia, where the lowest quality (87 can be considered quite low quality in terms of cocaine) is being shipped from but also involves the highest average price. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#interaction between price and ships_to, including quality...
cocaine %>%
  group_by(ships_to) %>%
  summarise(avg_quality = mean(quality), 
            avg_price = mean(cost_per_gram)) %>%
  ggplot(aes(reorder(ships_to, avg_price), avg_price)) +
  geom_col(aes(fill = avg_quality)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

Regarding the relationship with where the product is being shipped to, once again Australia (and this time New Zealand as well) stands out right at front for having the highest average price along with the lowest quality. This can warrant further specialized study as to whether there is a reason behind this strange Australian phenomena, or if it's just the way it is. 

Before progressing towards the modelling stage, the data needs to be cleaned up a bit by removing some unwanted variables. This is done by the following code chunk:

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
cocaine <- cocaine[, c(-1:-3, -6, -8, -10:-13, -15, -16)] #removing unwanted variables
cocaine[,4] <- as.factor(cocaine[,4]) #converting escrow to a factor as it is a categorical variable, not numeric
```


### Modelling

To begin modelling, first the data needs to be split into training and testing partitions, along with some feature engineering to make the data more suitable for the models. The data is normalized to fix the skewness problems observed earlier, and standardised to fix and range issues. All categorical variables are also dummy encoded. It should be noted that the total modelling process takes a while to run due to applying grid searches. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
set.seed(1234)
split <- initial_split(cocaine, prop = 0.7) #splitting the data by 70/30 train test split
train_cc <- training(split)
test_cc <- testing(split)

cc_recipe <- recipe(cost_per_gram ~ ., data = train_cc)

cc_recipe_steps <- cc_recipe %>%
  step_normalize(all_numeric()) %>% #applying normlazing to fix skewness 
  step_center(all_numeric()) %>% #standardise values
  step_scale(all_numeric()) %>% #standardise values
  step_dummy(all_nominal(), one_hot = FALSE) #convert all nominal to dummy

cc_recipe_prep <- prep(cc_recipe_steps, training = train_cc)

train_cc_rp <- bake(cc_recipe_prep, train_cc)
test_cc_rp <- bake(cc_recipe_prep, test_cc)
```

Having completed the feature engineering process, the first model is run. The KNN model is used only to serve as a benchmark for performance comparisons with the RF models to be run later on. Also, the resampling technique is defined as a 10-fold repeated cross-validation. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#Using repeated cross-validation...
cv <- trainControl(method = "repeatedcv",
                   number = 10,
                   repeats = 10)

#K-nn (benchmark)..
set.seed(1234)
knn_cc <- train(cost_per_gram ~ .,
                data = train_cc_rp,
                method = "knn",
                trControl = cv,
                metric = "RMSE")

knn_cc_pred <- predict(knn_cc, test_cc_rp) #Calculating predictions for later comparisons
knn_cc$results #Results
ggplot(knn_cc) #A simple visual to show performance with the different K values

```

The KNN model output shows that the optimal number of K neighbours was determined to be 5 which minimises the RMSE. The R squared seems to be good, but since this is just a benchmark, it needs to be compared with other values to be verified. 

Having run the benchmark, it is time to run the RF model. A grid search is used, using 4 different values of the parameter mtry. The mtry is defined as the number of variables available for the decision tree to split at each tree node. Since there are a significant number of variables involved, the mtry value is a useful parameter to tune to maximise the predictive power of the model. The ideal mtry value as per standard theory should be 17 (53/3), so a range of mtry values from 15 to 20 are selected to account for this. The code below runs the RF model. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#Random Forest...
set.seed(1234)

tunegrid_rf <- expand.grid(mtry = c(15:20)) #setting grid search 

rf_cc <- train(cost_per_gram ~ .,
               data = train_cc_rp,
               method = "rf",
               trControl = cv,
               tuneGrid = tunegrid_rf,
               metric = "RMSE",
               importance = TRUE)

rf_cc_pred <- predict(rf_cc, test_cc_rp)#Calculating predictions for later comparisons
rf_cc$results #Results 
```


The results of the RF model show that an mtry value of 19 provides the lowest RMSE, along with a higher R squared. These are both significant improvements over the benchmark KNN model. The performance of all mtry values can be visualised by the following plot:

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
ggplot(rf_cc)
```

The results however have the potential for further improvement. As there are numerous variables involved, there is scope of performing feature selection to significantly reduce the number of variables involved in order to improve accuracy. The following plot highlights the important variables as determined by the RF model, which will then be selected to run another iteration. Not surprisingly, grams is the most important for calculating cost_per_gram. It should be noted however that due to random sampling the list below might change. However, after running a few iterations, it was determined that from the top 20, about 17-18 variables remained in the top 20 consistently. Therefore, there should not be a significant difference in results. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=8}
ggplot(varImp(rf_cc))
```

Based on the variable importance plot, only the top 20 most important variables will be selected. After selecting, the same feature engineering process will be applied to create new training and testing data sets for the next RF model to run. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=8}
#Keeping only important variables...
cocaine_new <- subset(cocaine, 
                      select = c(grams, quality,
                                 ships_from_AU, rating,
                                 ships_from_NL, ships_from_GB,
                                 ships_from_EU, ships_to_GB, 
                                 ships_to_AU, ships_from_BR,ships_from_FR,
                                 ships_from_US, ships_to_NL,
                                 ships_from_DE, ships_from_IT,
                                 ships_to_WW, ships_to_EU,
                                 escrow,
                                 ships_to_DE,
                                 ships_to_AU, ships_to_US, cost_per_gram ))

split_new <- initial_split(cocaine_new, prop = 0.7)
train_new <- training(split_new)
test_new <- testing(split_new)

cc_recipe_new <- recipe(cost_per_gram ~ ., data = train_new)

cc_recipe_new_steps <- cc_recipe_new %>%
  step_normalize(all_numeric()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), one_hot = FALSE)

cc_recipe_new_prep <- prep(cc_recipe_new_steps, 
                           training = train_new)

train_varimp <- bake(cc_recipe_new_prep, train_new)
test_varimp <- bake(cc_recipe_new_prep, test_new)
```

The RF model is run again, but this time using different values for the mtry grid search as the total number of variables involved has dropped. By theory, the optimal number for mtry should be 7 (22/3), so a range from 5 to 10 will be applied in the grid search. 
```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=6}
#New RF model...

set.seed(1234)

tunegrid_rf_varimp <- expand.grid(mtry = c(5:10))

rf_cc_varimp <- train(cost_per_gram ~ .,
               data = train_varimp,
               method = "rf",
               trControl = cv,
               tuneGrid = tunegrid_rf_varimp,
               metric = "RMSE")

rf_cc_varimp$results #Results
rf_cc_varimp_pred <- predict(rf_cc_varimp, test_varimp) #For comparing the performance on test set for all 3 models

ggplot(rf_cc_varimp) #Plot to show change in RMSE across mtry values

```

The optimal mtry value for the RF model after subjecting it to feature selection is 9, where the RMSE value has dropped further down, and a very slight increase in the R-squared value. It is clear that removing some of the unimportant variables from the dataset improves the performance of the model while also reducing the computing power required. A comparison between the performance of all three models on the training data is shown below. Considering the RMSE as an accuracy metric, the KNN has performed the worst, while the feature selected tuned RF model performs the best. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=8}
summary(resamples(list(
  knn_model = knn_cc,
  rf_basic_model = rf_cc,
  rf_tuned_model = rf_cc_varimp
)))
```

However, before jumping to conclusions and selecting the best model, the performance of the predictions must also be compared using the testing data. These are shown below:

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=8}
KNN <- postResample(knn_cc_pred, test_cc_rp$cost_per_gram)
RF <- postResample(rf_cc_pred, test_cc_rp$cost_per_gram)
VarImp_RF <- postResample(rf_cc_varimp_pred, test_varimp$cost_per_gram)

data.frame("KNN" = KNN, "RF" = RF, "VarImp_RF" = VarImp_RF)
```

These results indicate that based on the error metrics, the feature selected RF model performs better.  


### Conclusion

The above analysis attempts to utilise the RF model to accurately predict cocaine prices based on the several features available. While performing feature selection and fine tuning the model is generally a good idea, care must be used to ensure that the successful accuracy metrics are not infact subject to overfitting, as seen above. If achieving solely the highest predictive performance was the goal here, then other predictive models could be utilised in a similar fashion, by using different iterations and applying feature engineering methods. 




