---
title: "Computer Prices with Simple Regression"
output: html_notebook
---



<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>This analysis involves a simple prediction problem. The dataset has been designed for basic data analysis, and is suitable for applying introductory predictive methods. The goal is to simply gain an understanding into the analytical process using two basic predictive models: Linear Regression and Multiple Adaptive Regression Splines (MARS).</p>
<p>The dataset contains some information about computers, with the dependant variable in this case being price, and other features including speed, RAM, screen etc. The data is freely avaibale from this link: <a href="https://www.kaggle.com/kingburrito666/basic-computer-data-set" class="uri">https://www.kaggle.com/kingburrito666/basic-computer-data-set</a></p>
<p>Before beginning, first the required libraries are loaded, along with the data itself.</p>
<pre class="r"><code>#Libraries
library(ggplot2)
library(magrittr)
library(rsample)
library(caret)
library(plyr)
#Load the Data
computers &lt;- read.csv(&quot;computers.csv&quot;, stringsAsFactors = TRUE)</code></pre>
</div>
<div id="data-preprocessing" class="section level3">
<h3>Data Preprocessing</h3>
<p>The next step before jumping into the analysis is to clean up the data a bit and fix any potential issues that may arise. Doing this requires some initial exploring.</p>
<pre class="r"><code>str(computers) #indicating the data type for each variable</code></pre>
<pre><code>## &#39;data.frame&#39;:    6259 obs. of  11 variables:
##  $ X      : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ price  : int  1499 1795 1595 1849 3295 3695 1720 1995 2225 2575 ...
##  $ speed  : int  25 33 25 25 33 66 25 50 50 50 ...
##  $ hd     : int  80 85 170 170 340 340 170 85 210 210 ...
##  $ ram    : int  4 2 4 8 16 16 4 2 8 4 ...
##  $ screen : int  14 14 15 14 14 14 14 14 14 15 ...
##  $ cd     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 2 1 1 1 ...
##  $ multi  : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ premium: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 1 2 2 2 2 2 2 ...
##  $ ads    : int  94 94 94 94 94 94 94 94 94 94 ...
##  $ trend  : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>sum(is.na(computers)) #whether there are any NA values</code></pre>
<pre><code>## [1] 0</code></pre>
<p>There are a few things that can be done to make this analysis a bit more simple. Some unnecessary features are removed, since there is no clear description about what these features actually mean with regards to computer pricing. This is more of an arbitrary judgement in this case. The features being removed are “X”, “ads”, and “trend”. Alongside this, the features “speed”, “ram” and “screen” are converted from integer to factor types as these variables are actually categorical instead of simple numerical values.</p>
<pre class="r"><code>computers.clean &lt;- computers[, -1] #removing X
computers.clean &lt;- computers.clean[, -10] #removing trend
computers.clean &lt;- computers.clean[, -9] #removing ads

computers.clean$speed &lt;- as.factor(computers.clean$speed) 
computers.clean$ram &lt;- as.factor(computers.clean$ram)
computers.clean$screen &lt;- as.factor(computers.clean$screen)</code></pre>
<p>A glimpse of the data can be helpful.</p>
<pre class="r"><code>head(computers.clean)</code></pre>
<pre><code>##   price speed  hd ram screen cd multi premium
## 1  1499    25  80   4     14 no    no     yes
## 2  1795    33  85   2     14 no    no     yes
## 3  1595    25 170   4     15 no    no     yes
## 4  1849    25 170   8     14 no    no      no
## 5  3295    33 340  16     14 no    no     yes
## 6  3695    66 340  16     14 no    no     yes</code></pre>
<p>It is also useful to convert the characters “yes” and “no” into 1 and 0 numbers respectively. While some predictive models do not require this conversion, other more simple methods do need numeric values. Therefore it is better to simply convert such variables to numeric, as opposed to risking an error.</p>
<pre class="r"><code>computers.clean$premium&lt;- revalue(computers.clean$premium, c(&quot;yes&quot;=1))
computers.clean$premium&lt;- revalue(computers.clean$premium, c(&quot;no&quot;=0))
computers.clean$multi&lt;- revalue(computers.clean$multi, c(&quot;yes&quot;=1))
computers.clean$multi&lt;- revalue(computers.clean$multi, c(&quot;no&quot;=0))
computers.clean$cd&lt;- revalue(computers.clean$cd, c(&quot;yes&quot;=1))
computers.clean$cd&lt;- revalue(computers.clean$cd, c(&quot;no&quot;=0))

summary(computers.clean) #Summarizing all the variables</code></pre>
<pre><code>##      price      speed            hd         ram       screen    cd      
##  Min.   : 949   25 : 566   Min.   :  80.0   2 : 394   14:3661   0:3351  
##  1st Qu.:1794   33 :2033   1st Qu.: 214.0   4 :2236   15:1992   1:2908  
##  Median :2144   50 : 994   Median : 340.0   8 :2320   17: 606           
##  Mean   :2220   66 :2028   Mean   : 416.6   16: 996                     
##  3rd Qu.:2595   75 : 122   3rd Qu.: 528.0   24: 297                     
##  Max.   :5399   100: 516   Max.   :2100.0   32:  16                     
##  multi    premium 
##  0:5386   0: 612  
##  1: 873   1:5647  
##                   
##                   
##                   
## </code></pre>
</div>
<div id="visualizing-and-exploring" class="section level3">
<h3>Visualizing and Exploring</h3>
<p>Following the preprocessing stage, the next step in this analysis is to visualize the features. This will provide any insights about the specific distributions and other aspects about the nature of each feature. As the dataset is quite simple, there is no expectation that any complicated insight will be generated. Still, it is useful to get a visual perspective.</p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = price)) +
  geom_histogram(bins = 30) + 
  theme_minimal() #histogram for Price</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-6-1.png" width="864" /></p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = price)) +
  geom_boxplot() + 
  theme_minimal() #boxplot for Price</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-6-2.png" width="864" /></p>
<p>Initial impressions of the histogram indicate a right-skewed distribution for computer prices. This is confirmed by the presence of some outliers indicated by high-priced computers. The outliers can be visualized by the boxplot. While generally it is recommended to transform the dependent variable in order to reduce skewness, here it is ignored as firstly it is really not that severe, and for the purpose of this exercise it is not necessary to strictly ensure Gaussian distributions as this is more about describing the process of analysis, as opposed to analytical accuracy.</p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = hd)) +
  geom_histogram(bins = 20) + 
  theme_minimal() #historgram for HD</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-7-1.png" width="864" /></p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = hd)) +
  geom_boxplot() +
  theme_minimal() #boxplot for HD</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-7-2.png" width="864" /></p>
<p>Similarly skewed distributions and outliers are observed for the HD variable as well. As RAM and Screen are categorical variables, simple bar plots can be used to visualize them. A useful insight into these variables is the imbalanced count between them, with large RAM and Screen size accounting for a small proportion of the data. This can be a problem for predictive models, especially with small sample sizes such as this data, as there are not enough observations for the model to provide a robust prediction at these specific categories. This problem can be solved by random sampling techniques.</p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = ram)) +
  geom_bar(aes(fill = ram)) +
  theme_minimal() #bar plot for RAM</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-8-1.png" width="864" /></p>
<pre class="r"><code>computers.clean %&gt;%
  ggplot(aes(x = screen)) +
  geom_bar(aes(fill = screen)) +
  theme_minimal() #bar plot for screen size</code></pre>
<p><img src="/blog/Computers_files/figure-html/unnamed-chunk-8-2.png" width="864" /></p>
<p>Finally, proportion tables can also be used to visualize and understand the remaining variables; premium, multi and cd. Disproportionate categories can be seen in the Premium and Multi variables as well, while CD remains somewhat balanced.</p>
<pre class="r"><code>prop.table(table(computers.clean$premium)) * 100</code></pre>
<pre><code>## 
##        0        1 
##  9.77792 90.22208</code></pre>
<pre class="r"><code>prop.table(table(computers.clean$multi)) * 100</code></pre>
<pre><code>## 
##        0        1 
## 86.05208 13.94792</code></pre>
<pre class="r"><code>prop.table(table(computers.clean$cd)) * 100</code></pre>
<pre><code>## 
##       0       1 
## 53.5389 46.4611</code></pre>
</div>
<div id="modelling" class="section level3">
<h3>Modelling</h3>
<p>Having completed the visualizing and exploring phase, the next step is the actual modelling. For this analysis, two models are being used, as mentioned earlier. The simple Linear Regression Model will first be applied, serving as the benchmark performer. Following this, the MARS model will be applied to observe if there is any improvement in accuracy.</p>
<p>Before moving on to the actual models, the data first needs to be split into training and testing partitions. A 65/35 split is used here, as the data size is small and this split reserves a fair portion for testing to avoid overfitting models.</p>
<pre class="r"><code>set.seed(1234)
split &lt;- initial_split(computers.clean, prop = 0.65)
comp_train &lt;- training(split) #4069 rows
comp_test &lt;- testing(split) #2190 rows</code></pre>
<p>The next step is choosing a resampling strategy. Repeated cross-validation generally works quite well to improve accuracy and performance. However, on very large datasets this will obviously be more computationally taxing than regular cross-validation or bootstrapping.</p>
<pre class="r"><code>cv &lt;- trainControl(
  method = &quot;repeatedcv&quot;,
  number = 20,
  repeats = 10
) #20-fold cross validation with 10 repititions. Seems a bit overkill but it&#39;s okay. </code></pre>
<p>The modelling process can now begin. The caret() package is being used for model training and testing. Below is the Linear Model being implemented:</p>
<pre class="r"><code>set.seed(1234)
comp_model_lm &lt;- train(form = price ~ .,
                     data = comp_train,
                     method = &quot;lm&quot;,
                     trControl = cv) 

comp_model_lm #Linear Model</code></pre>
<pre><code>## Linear Regression 
## 
## 4069 samples
##    7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (20 fold, repeated 10 times) 
## Summary of sample sizes: 3866, 3865, 3867, 3865, 3865, 3865, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   386.7789  0.5620502  288.9718
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Initial impressions of the LM model state that the model performed somewhat average. The R-square is nothing spectacular. As this is simply a benchmark model, the RMSE value and R square will be used as a comparison with future model performances.</p>
<p>The next step is applying the MARS model. This model may work better than the standard linear model if there is a lack of a strictly linear relationship between the predictors and the dependant variable. A basic implementation of the MARS model is being applied here, without any hyperparameter tuning or grid searches. This will utilise default values for interaction complexity (degree) and find the number of terms to retain (nprune). It should be noted that the number of degrees selected is very unlikely to be above 3 as there tends to be very little benefit in performance beyond this number.</p>
<pre class="r"><code>set.seed(1234)

comp_model_mars &lt;- train(form = price ~ .,
                     data = comp_train,
                     method = &quot;earth&quot;,
                     metric = &quot;RMSE&quot;,
                     trControl = cv)

comp_model_mars #MARS model</code></pre>
<pre><code>## Multivariate Adaptive Regression Spline 
## 
## 4069 samples
##    7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (20 fold, repeated 10 times) 
## Summary of sample sizes: 3866, 3865, 3867, 3865, 3865, 3865, ... 
## Resampling results across tuning parameters:
## 
##   nprune  RMSE      Rsquared   MAE     
##    2      529.6608  0.1785759  410.7668
##    8      405.0407  0.5195990  310.7784
##   15      374.6545  0.5894418  282.7044
## 
## Tuning parameter &#39;degree&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were nprune = 15 and degree = 1.</code></pre>
<p>The three metrics used to compare the two models are the RMSE (Root Mean Squared Error), R-squared and MAE (Mean Absolute Error). Generally the RMSE is used more frequently as it penalizes larger errors more than the MAE, but in this case all three are used.</p>
<pre class="r"><code>summary(resamples(list(
  lm_model = comp_model_lm,
  mars_model = comp_model_mars
)))</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamples(list(lm_model = comp_model_lm,
##  mars_model = comp_model_mars)))
## 
## Models: lm_model, mars_model 
## Number of resamples: 200 
## 
## MAE 
##                Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## lm_model   223.9756 279.1161 290.2203 288.9718 299.6090 338.5717    0
## mars_model 217.0863 271.5805 283.3691 282.7044 292.9858 327.0437    0
## 
## RMSE 
##                Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## lm_model   280.9617 369.5891 386.8543 386.7789 402.4727 456.9774    0
## mars_model 272.7934 357.9470 373.0703 374.6545 391.0237 442.5427    0
## 
## Rsquared 
##                 Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## lm_model   0.4207443 0.5296888 0.5642487 0.5620502 0.5950278 0.7379837    0
## mars_model 0.4538896 0.5600248 0.5954861 0.5894418 0.6178997 0.7520546    0</code></pre>
<p>It can be seen that the average RMSE and MAE are both lower for the MARS model as opposed to the Linear Model. The average R-square for the MARS model is also higher. However, it should be noted that these differences are very marginal. Also, these tests are done on the training sets, and the metrics need to be calculated on the testing partition as well to assess whether there was any overfitting.</p>
<pre class="r"><code>lm_pred &lt;- predict(comp_model_lm, comp_test) #predicting using the trained model on the test sets
mars_pred &lt;- predict(comp_model_mars, comp_test)

postResample(lm_pred, comp_test$price)</code></pre>
<pre><code>##        RMSE    Rsquared         MAE 
## 384.5036526   0.5527321 292.8023770</code></pre>
<pre class="r"><code>postResample(mars_pred, comp_test$price)</code></pre>
<pre><code>##        RMSE    Rsquared         MAE 
## 377.7778678   0.5679313 284.0097597</code></pre>
<p>The above results again indicate that the MARS model performs better, yet still only being a marginal improvement. It is also a good indication that there is no significant overfitting, as the RMSE and R-square values are fairly close to each other for both training and testing sets.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>The above analysis can be considered a simplified version of the analytical process, starting from data cleaning and processing, moving on to visualizing and exploration and ending at the modelling phase. From the modelling phase, it was determined that the MARS model performs better at predicting the computer prices. However, this result should not be taken for granted, at least not without any further exploration. It was seen that some features of the data were skewed, and training models on such skewed data without correcting for this can lead to unreliable testing results, especially in models such as the simple Linear Model. Along with this, feature testing can also be conducted to determine which features are more important for the predictions. Also, it is useful to run numerous iterations of the models, specifically with hyperparameter tuning as this tends to produce more accurate results.</p>
</div>
