---
title: "Epileptic Seizure Recognition with Deep Learning"
output:
  html_document:
    df_print: paged
---

### Introduction

This project attempts to classify whether a patient is suffering from an epileptic seizure or not based on certain known features. The dataset was freely obtained from the following link: https://www.kaggle.com/harunshimanto/epileptic-seizure-recognition. To summarise the data, the features comprise of individual data points of an EEG recording, monitoring a subject's brain activity at different points in time. Further details about the specifications of these variables can be found via the given link. The data consists of 178 explanatory variables, labeled X1, X2,......,X178, and the target variable is simply called y. The target variable comprises of 5 classes, highlighting the following information:

5 - eyes open, meaning when they were recording the EEG signal of the brain the patient had their eyes open

4 - eyes closed, meaning when they were recording the EEG signal the patient had their eyes closed

3 - they identified where the region of the tumor was in the brain and recorded the EEG activity from the healthy brain area

2 - they recorded the EEG from the area where the tumor was located

1 - recording of seizure activity

To summarise, only class 1 indicates any presence of seizures, while the other 4 classes can be considered as the absence of seizures. With 178 features, there are 11500 rows of information, making this a reasonable sized dataset for applying deep learning techniques. The Tensorflow library is utilised here to construct a simple deep neural network for classification (multi-layer perceptron). In order to compare the performance of the neural network with other models, a series of ensemble methods and a logistic regression is also applied to examine any change in performance. This analysis is carried out in Python. 

Before beginning, first the required libraries are loaded. 

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#importing libraries

import matplotlib
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from pycaret.classification import *
import warnings
warnings.filterwarnings("ignore")
```


### Exploring and Processing

Before moving forward to building the neural network, first some preprocessing and exploring of the data is carried out. Classes 2,3,4 and 5 are replaced with 0, indicating a singular class representing no seizure activity. This simplifies the data a bit as it is now a simple binary classification problem. However, with this conversion, there is now a severe class imbalance, which is shown below.   

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#Initial preprocessing and exploring

#for reproducibility
np.random.seed(1234)

#Load Data
seizure = pd.read_csv('Epileptic-Seizure-Recognition.csv')

#remove first column
seizure = seizure.loc[:, ~seizure.columns.str.contains('^Unnamed')]

#replace all other classes with 0 to indicate no seizure
seizure.y = seizure.y.replace({2:0, 3:0, 4:0, 5:0})

```


![Class Imbalance Plot](/images/Class_Imbalance.png)



In such a case of imbalanced classes, it is important that there should be some attempt to balance the data, otherwise any performance metrics generated by the models will be highly misleading as the model will be heavily biased towards to the majority class. As there is ample data available, undersampling is a useful technique that can be used here to randomly select observations from the majority class (0) equal to the number of minority class (1) observations. From 11500 original observations, there are now 4600 observations left to build the model on, which is still reasonable, as the number of features is still well below the number of observations. 

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#create input and output categories for tensorflow
X = seizure.loc[:, seizure.columns != 'y']
y = seizure.loc[:, 'y']

#scale the independent variables and convert output to categorical
X = sklearn.preprocessing.scale(X)
y = to_categorical(y)

#balance classes by random undersampling
undersample = RandomUnderSampler(sampling_strategy = 'majority')
X_under, y_under = undersample.fit_resample(X, y)
```


### Building the NN model via Tensorflow

Having processed the data, it is now time to build the neural network model. Initially, the data is split into training and testing partitions (80/20 split). To reach a reasonable level of depth, 3 hidden layers are used, each with an approximately proportional decrease in the number of neurons available to allow smooth convergence from 178 input nodes to 2 output nodes (representing the 2 classes). Since this is a classification task, the sigmoid activation function is generally preferred for the output layer, and accuracy is used as the performance metric. The number of epochs is set to let the model undergo 100 iterations over the entire dataset. 

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#train_test split
X_train, X_test, Y_train, Y_test = train_test_split(X_under, y_under, 
                                                    test_size = 0.2)

#A deep NN model
model_d = Sequential()
model_d.add(Dense(120, activation = 'relu', input_dim = 178))
model_d.add(Dense(90, activation = 'relu'))
model_d.add(Dense(40, activation = 'relu'))
model_d.add(Dense(1, activation = 'sigmoid'))
model_d.compile(optimizer = 'adam', loss = 'binary_crossentropy',
                metrics = ['accuracy'])
model_d.fit(X_train, Y_train, epochs = 100, verbose = 0)
```


Once the model is run, the performance over both the training and testing set is noted. 

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#Training set accuracy
scores_d = model_d.evaluate(X_train, Y_train, verbose=False)
print("Training Accuracy: %.2f%%\n" % (scores_d[1]*100))
```


```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#Testing set accuracy
scores_d = model_d.evaluate(X_test, Y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores_d[1]*100))
```


The training set achieves a spectacular 100% accuracy. However, this may be too good to be true, as overfitting is a possibility. The testing set accuracy confirms this, reaching a lower value of about 95%. This is still a very good result from just a simple model put together. The performance is further emphasised by the ROC curve, which closely resembles that of a perfect model. Perhaps with further hyperparameter tuning, a more detailed and potentially even deeper model can be created to improve accuracy even further.

![ROC for the Neural Network](/images/ROC_DNN.png)


### Other Methods

Before prematurely deciding that deep neural networks are the greatest thing ever and should be used everywhere all the time, it is worth investigating whether similar results can be achieved with other models. This is because neural networks are generally one of the least interpretable methods used, and while obtaining the highest possible accuracy is a worthy goal, understanding what the model is actually even doing is also important. Good model understanding will enable the right method being used in the right situation, as opposed to blindly plugging in the most complex method. 

In order to make this comparison, the PyCaret package is used, as it implements several popular machine learning methods quickly and easily. First, the data is combined to form a single dataframe once again for the package to process. 

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#combine X_under and y_under into a single array
seizure_arr = np.concatenate([X_under,y_under], axis = 1)

#convert the array back into a data frame
seizure_df = pd.DataFrame(seizure_arr)

#setting original column names
seizure_df.columns = seizure.columns

#had to apparently convert to categorical again for the models to run
seizure_df.y = to_categorical(seizure_df.y)

```


After some brief preprocessing, the PyCaret package is initialized. Do note that the code from this point is shown with a ' # ' symbol preceding each command. This is only done as there are some rendering issues between this package and R Markdown (on which these reports are made). The ' # ' symbol is not necessary if the actual code is being run.

The environment is setup, with a default train-test split of 70/30 (refer to the PyCaret documentation for further details). The two methods being used here are the simple Logistic Regression as a benchmark, and a series of ensemble techniques including Extreme Gradient Boosting and Random Forests. The accuracy results for each method are shown below. 


```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#setup for pycaret
seizure_pc = setup(seizure_df, target = 'y', html = False, silent = True, verbose=False)
```

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#run simple logistic regression as benchmark
seizure_logistic = create_model('lr')
```

```{python, engine.path = 'C:\\Users\\faiza\\Anaconda3\\python.exe'}
#run ensemble models
seizure_ensemble = compare_models(whitelist = models(type='ensemble').index.tolist(), fold = 3)
```

The Logistic Regression performed the worst, reaching a mean accuracy value of only approximately 60%. This is also illustrated by its ROC curve shown below. 

![ROC for logistic regression](/images/ROC_log.png)

On the other hand, the Extra Tree Classifier from the ensemble methods has performed quite well, with an accuracy level almost equivalent to that of the Neural Network. This is illustrated by the equally similar ROC curve generated below. 

![ROC for Ensembles](/images/ROC_Ensemble.png)

### Conclusion

The above analysis has two important highlights. Firstly, deep learning techniques are incredibly powerful for use on complex data due to their inherent flexibility. With time as packages such as Tensorflow are being continously developed, implementing neural networks is gradually becoming easier and more efficient. Secondly, it should be noted that just because they are complex, neural networks may not always be the answer, as just like any other model, they have their shortcomings (adversarial machine learning comes to mind). Also, it is highly likely that given a particular use-case, there may be a number of models that provide equally promising results. The decision regarding which model to use therefore depends on the particular situation and goal of the analysis itself. 

