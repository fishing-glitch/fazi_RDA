---
title: "Semi-Supervised Learning with Pseudo Labelling"
output: html_notebook
---

### Introduction

Machine learning techniques for predictive analytics of data can be divided in 3 main categories: supervised, unsupervised, and semi-supervised. While supervised and unsupervised techniques work with labelled and unlabelled data respectively, semi-supervised learning uses both labelled and unlabelled data simultaneously to improve the performance of any model being trained. This method is especially useful in situations where only a limited amount of labelled data is present for training, while there is also unlabelled data available. 

The aim of this analysis is to demonstrate a simple implementation of semi-supervised learning on tabular data, using a technique called pseudo labelling. Improvements in model performance will be shown using this method, while also discussing some considerations that should be made during its implementation. Although the dataset used here is quite simple, pseudo labelling can be and has been successfully applied to more complex datasets such as images. 

The analysis is completed in R. All packages and data are loaded below. The dataset is related to electric power quality and was obtained freely from the following link: https://www.kaggle.com/jaideepreddykotla/powerqualitydistributiondataset1.

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
#load libraries
library(tidyverse)
library(rsample)
library(recipes)
library(caret)
library(pROC)
library(reshape2)

#load data
pq <- read.csv('PowerQualityDistributionDataset1.csv')
```

### Data Processing

As there are not many details available describing what the data is specifically about from the source, not much can be said about the features and their relationship with the target variable. However, in the context of this analysis, understanding these relationships is not important as the aim here is to demonstrate the effects of pseudo labelling as a method of predictive modelling. However, the data will still be briefly explored and processed. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
#data dimensions
data.frame(nrow(pq), ncol(pq))
```
The dataset is fairly large, as shown above. As pseudo labelling is applied on classification problems, the target variable in this case is divided into 5 different classes of power quality. To make this problem simpler, this is converted into a binary classification problem, with classes labelled 3 and below placed under a new category called 'Low', while classes 4 and 5 are placed under the category 'High'. From the 130 features, the first one is only an index column, so this is removed. Excluding the target variable, there are 128 features, indicating a high number of dimensions in the data. To make the modelling process more efficient, dimension reduction will also be carried out to reduce the number of features. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq <- pq[,-1] #remove index column

colnames(pq)[129] <- 'y' #rename target variable

pq <- pq %>%
  mutate(y = ifelse(y <=3, 'Low', 'High')) #converting to binary classification

pq$y <- as.factor(pq$y) #applying suitable data type
```

The output variable is visualized below. The class imbalance, although present, is not severe enough to be detrimental to performance. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
ggplot(pq, aes(y)) +
  geom_bar(fill = 'black') +
  geom_text(aes(label = ..count..), stat = 'count',
            vjust = -0.8) +
  xlab('Low/High Power Quality') +
  theme_minimal()
```

The explanatory variables in this dataset are quite similar to each other in terms of how they are distributed and scaled. Boxplots for the first 10 variables from 128 shows this similarity. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
temp_table <- melt(pq[,1:10]) #selecting first 10 variables

ggplot(temp_table, aes(x = variable, y = value)) +
  geom_boxplot(fill = 'grey') +
  theme_minimal()

rm(temp_table)
```

It should be noted that this data is entirely labelled. As pseudo labelling requires unlabelled data, this dataset will be split into different partitions, and one partition will have its labels removed in order to simulate unlabelled data. The data is initially split into 80% training and 20% testing data. From the training data, 30% is split for the actual labelled dataset to be used, and 70% is kept aside for the 'unlabelled' dataset. The testing set is split into half, with 50% going for a validation set and 50% for an overall testing set. The use of a validation set is highly recommended in this analysis as it is an iterative process with some parameter tuning, and using a validation set in this case will prevent the risk of data leakage. All the partitions are created ensuring that the original class balance shown earlier is maintained in each set. The overall test set will be used only for the initial run and the final run to showcase the overall improvement in performance. 

Once all these partitions are created, dimension reduction is applied via PCA (Principal Component Analysis). To explain it briefly, PCA essentially combines variables in such a way that each 'component' aims to explain the maximum amount of variation in a dataset, while being independent to each other. Here, 3 components are extracted from the PCA procedure, thus significantly dropping the overall number of features. PCA is carried out on all the partitions. Finally, the labels from the unlabelled partition are removed, and the data is now ready for the pseudo labelling process. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
#First Partition
set.seed(1234)
split <- initial_split(pq, prop = 0.80, strata = 'y')
pq_train <- training(split)
pq_test <- testing(split)

#Second Partition
set.seed(1234)
split_2 <- initial_split(pq_train, prop = 0.3, strata = 'y')
pq_train_lab <- training(split_2)
pq_train_unlab <- testing(split_2)

#Third Partition
set.seed(1234)
split_3 <- initial_split(pq_test, prop = 0.5, strata = 'y')
pq_testing <- training(split_3)
pq_validation <- testing(split_3)

#Applying PCA
pq_recipe <- recipe(y ~ ., data = pq_train_lab) %>%
  step_pca(all_numeric(), num_comp = 3) %>%
  prep(training = pq_train_lab)

pq_train_lab <- bake(pq_recipe, pq_train_lab)
pq_train_unlab <- bake(pq_recipe, pq_train_unlab)
pq_testing <- bake(pq_recipe, pq_testing)
pq_validation <- bake(pq_recipe, pq_validation)

#removing unnecessary data
rm(split, split_2, split_3, pq_train, pq_test, pq_recipe, pq)

pq_train_lab <- pq_train_lab[,c(2,3,4,1)]
pq_train_unlab <- pq_train_unlab[,-1] #removing labels from unlabelled data
pq_testing <- pq_testing[,c(2,3,4,1)]
pq_validation <- pq_validation[,c(2,3,4,1)]
```

With the data ready, the modelling phase can now begin.

### Initial Baseline Regression

The model chosen here is a simple logistic regression, to avoid any complications associated with more complex models. However, pseudo labelling can be applied to any model. An initial baseline performance is noted by testing the model on both the testing and the validation set. For the iterations however only the validation set will be used. 5-fold Cross-Validation is used as the sampling method, and the AUC (Area Under the Curve) of the ROC (Receiver Operating Characteristic) curve is used as the performance metric to monitor. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
cv <- trainControl(method = 'cv', number = 5,
                   classProbs = TRUE, summaryFunction = twoClassSummary) #setting resampling method.

set.seed(1234)
logistic_model <- train(
  y ~ .,
  data = pq_train_lab,
  mthod = 'glm',
  family = 'binomial',
  trControl = cv,
  metric = 'ROC'
) #baseline logistic model

preds_test_base <- predict(logistic_model, pq_testing, type = 'prob')
round(roc(pq_testing$y, preds_test_base$High)$auc,6) #measuring auc on test set
```

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
preds_base <- predict(logistic_model, pq_validation, type = 'prob')
round(roc(pq_validation$y, preds_base$High)$auc, 6) #measuring auc on validation set
```

The results from the validation (0.98828) and test (0.987686) sets highlight an already strong performing model, so any improvements observed may be relatively marginal (3rd or 4th decimal place). However, the amount of improvement is highly dependent on the data itself (in terms of quality and amount), where in some cases the performance improvement can be much larger or smaller. With the baseline measurements noted, the first iteration can be run. 
 
```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
rm(preds_base, preds_test_base)
```

### First Iteration

To describe the process as simply as possible, pseudo-labelling basically involves running a trained model on an unlabelled dataset and generating class-specific probabilities for each class being predicted. As this is a binary classification problem, here the class probabilities will be for 'High' and 'Low'. Class probabilities is referring to the percentage probability generated by the model for each unlabelled observation. This probability signifies the confidence the model has in a specific label it has predicted for each observation.   

Once these class probabilities from the unlabelled dataset are generated, observations with probabilities above a certain threshold are extracted without replacement from the unlabelled dataset. These observations, along with their assigned labels, are then combined with the original training data. The model is then trained again on the evolved training data, and the steps are run for multiple iterations until no further observations that meet the required threshold are found, or until performance stops improving. 

There are a couple of parameters to note here. Firstly, the threshold is important in selecting the optimal observations to combine. Different levels of performance have been observed for different thresholds, and thus tuning this value is required. In general, a higher threshold (over 0.8) is preferred as it implies more confidence by the model. Secondly, the label which is selected to combine with the training data should also be carefully chosen. For instance, in this analysis, the class 'High' is in the minority. Therefore, only the observations where the model is confident that the class label is 'High' are selected, as this reduces the already existing class imbalance present in the datasets. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq_train_unlab$y <- predict(logistic_model, pq_train_unlab, type = 'prob')$High #run model on unlabelled data
best_1st_iter <- subset(pq_train_unlab, y >= 0.85) #extract observations greater than 0.85
nrow(best_1st_iter)
```

In this first iteration, the model is run on the unlabelled data, and observations equal to and above the threshold of 0.85 are selected. This amounts to 2279 total observations extracted, all with the class label 'High'. It should be noted that 0.85 was selected as it gave the best performance from the choice of 0.75, 0.85 and 0.95. It is possible to select a more precise threshold, but for this case 0.85 is sufficient. The 2279 observations are then removed from the unlabelled data and combined with the training dataset. The model is then trained on the new training data and its performance on the validation set is noted.

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq_train_unlab <- subset(pq_train_unlab, y < 0.85) #remove selected observations from unlabeled data
pq_train_unlab <- pq_train_unlab[,-4] #removing remaining labels

best_1st_iter$y <- 'High'
best_1st_iter$y <- as.factor(best_1st_iter$y)

pq_train_lab <- rbind(pq_train_lab, best_1st_iter) #combine training and selected observations

set.seed(1234)
logistic_model <- train(
  y ~ .,
  data = pq_train_lab,
  mthod = 'glm',
  family = 'binomial',
  trControl = cv,
  metric = 'ROC'
) #run model

preds_1st <- predict(logistic_model, pq_validation, type = 'prob')

round(roc(pq_validation$y, preds_1st$High)$auc,6) #generate first iteration auc

rm(preds_1st,best_1st_iter)
```

Combined to the baseline performance with the validation set (0.98828), the new value of 0.988651 is a clear improvement in results. This implies that the first iteration was successful, and a second iteration can be run. 

### Second Iteration

The exact same steps as in the first iteration are applied. The only difference is this time the threshold is shifted to 0.95, as once again through tuning, this threshold proved to have the best results. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq_train_unlab$y <- predict(logistic_model, pq_train_unlab, type = 'prob')$High
best_2nd_iter <- subset(pq_train_unlab, y >= 0.95)
nrow(best_2nd_iter)
```

The number of selected observations has now drastically fallen compared to the first iteration, with only 27 selected. While this is partly due to the higher threshold, it also indicates that the total number of positive iterations may only be limited to 2 or 3 in this case. The observations are combined with the training data, and the model is run again. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq_train_unlab <- subset(pq_train_unlab, y < 0.95)
pq_train_unlab <- pq_train_unlab[,-4]

best_2nd_iter$y <- 'High'
best_2nd_iter$y <- as.factor(best_2nd_iter$y)

pq_train_lab <- rbind(pq_train_lab, best_2nd_iter)

set.seed(1234)
logistic_model <- train(
  y ~ .,
  data = pq_train_lab,
  mthod = 'glm',
  family = 'binomial',
  trControl = cv,
  metric = 'ROC'
)

preds_2nd <- predict(logistic_model, pq_validation, type = 'prob')

round(roc(pq_validation$y, preds_2nd$High)$auc,6) #second iteration auc

rm(preds_2nd, best_2nd_iter)
```

Another improvement has been observed in the AUC score, thus rendering the second iteration successful as well. A third iteration is then run. 

### Third Iteration

A third iteration is run, with a new threshold of 0.98, thus selecting only the observations with the best confidence. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
pq_train_unlab$y <- predict(logistic_model, pq_train_unlab, type = 'prob')$High
best_3rd_iter <- subset(pq_train_unlab, y >= 0.98)
nrow(best_3rd_iter)
```

However, at this threshold, there are no longer any observations available. At this point, the iterations can be stopped, and the final model can now be used further. Lowering the threshold may yield more observations, but that implies lowering the confidence even further and this will likely not lead to further model performance, especially in this case where performance increments are already quite marginal. The density plot for the probabilities predicted by the model also indicates that majority of the observations have quite low confidence attached to it. This makes further iterations pointless. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
ggplot(pq_train_unlab, aes(x = y)) +
  geom_density(fill = 'black') +
  theme_minimal()
```

### Final Assessment

The final trained model is then run on the separate test set seen at the start, to observe if the performance improvement is consistent. 

```{r message=FALSE, warning=FALSE, fig.width=9, fig.height=7}
preds_test <- predict(logistic_model, pq_testing, type = 'prob')

round(roc(pq_testing$y, preds_test$High)$auc,6)
```

Compared to the baseline value of 0.987686, the final trained model now generates a higher AUC of 0.9884, thus indicating that the process has definitely improved the overall results. 

It was mentioned earlier that pseudo-labelling can be especially useful in cases where the labelled dataset available for training may be highly limited but sufficient unlabelled data is available. In the dataset used here, this was not really the case as the amount of data was quite sufficient and the individual structure of each feature did enable excellent performance by default. However, this does not imply that the use of pseudo-labelling results in limited performance increments. Instead, it should be observed that for this method to work, there are some key parameters that need to be carefully considered, including the actual data itself, in order to maximize the performance returns observed.


