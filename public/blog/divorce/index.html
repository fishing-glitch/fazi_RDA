<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Classifying Divorce | Fazi_RDA</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />

  
  
    
 
  
  
  
  
  
  
    
    <link rel="stylesheet" href="/css/post.min.3b28d14676e4769849164baf362f2b0aa069ab25702fef98f0c4227cb68d74cd.css" integrity="sha256-OyjRRnbkdphJFkuvNi8rCqBpqyVwL&#43;&#43;Y8MQifLaNdM0="/>
  
    
    <link rel="stylesheet" href="/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "\/"
      },
      "articleSection" : "blog",
      "name" : "Classifying Divorce",
      "headline" : "Classifying Divorce",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "0001",
      "datePublished": "0001-01-01 00:00:00 \u002b0000 UTC",
      "dateModified" : "0001-01-01 00:00:00 \u002b0000 UTC",
      "url" : "\/blog\/divorce\/",
      "wordCount" : "2711",
      "keywords" : ["Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" role="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">projects</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Classifying Divorce</h1>
            <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="post__date"
            > </time>
          </header>
          <article class="post__content">
              



<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>The following analysis is a classification problem, where the goal is to try and predict divorce based on a “Divorce Predictors Scale”. The data was developed based on a psychology study named Gottman couples therapy, and was freely obtained via <a href="http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set" class="uri">http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set</a>. The link to the original study from which this data emerged is <a href="https://dergipark.org.tr/en/pub/nevsosbilen/issue/46568/549416" class="uri">https://dergipark.org.tr/en/pub/nevsosbilen/issue/46568/549416</a>. The aim here is to utilise the answers by couples to 54 questions regarding their marriage to try and predict whether the marriage is intact or if divorce is imminent.</p>
<p>Loading the data and required libraries:</p>
<pre class="r"><code>#Libraries...
library(corrplot)
library(caret)
library(tidyverse)
library(rsample)
library(ROCR)
library(vip)

#Loading data...
divorce &lt;- read.csv(&quot;divorce.csv&quot;, sep = &quot;;&quot;)</code></pre>
</div>
<div id="visualizing-and-exploring" class="section level3">
<h3>Visualizing and Exploring</h3>
<p>Before progressing forward, it may be helpful to list all the questions as a quick reference if required. The 54 questions are as follows:</p>
<p>Status (dependent variable) - describes whether divorced (1) or married (0)</p>
<ol style="list-style-type: decimal">
<li>If one of us apologizes when our discussion deteriorates, the discussion ends.</li>
<li>I know we can ignore our differences, even if things get hard sometimes.</li>
<li>When we need it, we can take our discussions with my spouse from the beginning and correct it.</li>
<li>When I discuss with my spouse, to contact him will eventually work.</li>
<li>The time I spent with my wife is special for us.</li>
<li>We don’t have time at home as partners.</li>
<li>We are like two strangers who share the same environment at home rather than family.</li>
<li>I enjoy our holidays with my wife.</li>
<li>I enjoy traveling with my wife.</li>
<li>Most of our goals are common to my spouse.</li>
<li>I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.</li>
<li>My spouse and I have similar values in terms of personal freedom.</li>
<li>My spouse and I have similar sense of entertainment.</li>
<li>Most of our goals for people (children, friends, etc.) are the same.</li>
<li>Our dreams with my spouse are similar and harmonious.</li>
<li>We’re compatible with my spouse about what love should be.</li>
<li>We share the same views about being happy in our life with my spouse</li>
<li>My spouse and I have similar ideas about how marriage should be</li>
<li>My spouse and I have similar ideas about how roles should be in marriage</li>
<li>My spouse and I have similar values in trust.</li>
<li>I know exactly what my wife likes.</li>
<li>I know how my spouse wants to be taken care of when she/he sick.</li>
<li>I know my spouse’s favorite food.</li>
<li>I can tell you what kind of stress my spouse is facing in her/his life.</li>
<li>I have knowledge of my spouse’s inner world.</li>
<li>I know my spouse’s basic anxieties.</li>
<li>I know what my spouse’s current sources of stress are.</li>
<li>I know my spouse’s hopes and wishes.</li>
<li>I know my spouse very well.</li>
<li>I know my spouse’s friends and their social relationships.</li>
<li>I feel aggressive when I argue with my spouse.</li>
<li>When discussing with my spouse, I usually use expressions such as ‘you always’ or ‘you never’ .</li>
<li>I can use negative statements about my spouse’s personality during our discussions.</li>
<li>I can use offensive expressions during our discussions.</li>
<li>I can insult my spouse during our discussions.</li>
<li>I can be humiliating when we discussions.</li>
<li>My discussion with my spouse is not calm.</li>
<li>I hate my spouse’s way of open a subject.</li>
<li>Our discussions often occur suddenly.</li>
<li>We’re just starting a discussion before I know what’s going on.</li>
<li>When I talk to my spouse about something, my calm suddenly breaks.</li>
<li>When I argue with my spouse, ı only go out and I don’t say a word.</li>
<li>I mostly stay silent to calm the environment a little bit.</li>
<li>Sometimes I think it’s good for me to leave home for a while.</li>
<li>I’d rather stay silent than discuss with my spouse.</li>
<li>Even if I’m right in the discussion, I stay silent to hurt my spouse.</li>
<li>When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger.</li>
<li>I feel right in our discussions.</li>
<li>I have nothing to do with what I’ve been accused of.</li>
<li>I’m not actually the one who’s guilty about what I’m accused of.</li>
<li>I’m not the one who’s wrong about problems at home.</li>
<li>I wouldn’t hesitate to tell my spouse about her/his inadequacy.</li>
<li>When I discuss, I remind my spouse of her/his inadequacy.</li>
<li>I’m not afraid to tell my spouse about her/his incompetence.</li>
</ol>
<p>As all the answers are categorical by nature (a single value between 0 and 4), the variables need to be converted to a factor first before moving forward with the analysis.</p>
<pre class="r"><code>divorce[,1:55] &lt;- lapply(divorce[,1:55], factor) # convert all to factor
colnames(divorce)[55] &lt;- &quot;Status&quot; #change dependent variable name

summary(divorce$Status) #a summary of the dependent variable</code></pre>
<pre><code>##  0  1 
## 86 84</code></pre>
<p>From the summary values above, it is clear that the two classes (1 and 0) are almost equal in quantity, therefore there is no class imbalance problem. This luckily makes the analysis simpler to perform.</p>
<p>To further understand the independent variables, a correlation plot is developed to see if any of the questions provide approximately the same information to the data. It should be noted that ordinarily with categorical variables the chi square of Cramer’s V methods are used to measure association strength. However, since these categorical variables are ordinal, the Spearman’s coefficient may also be used.</p>
<pre class="r"><code>divorce_num &lt;- divorce[,1:54] #temporary dataframe created
divorce_num[,1:54] &lt;- sapply(divorce_num[,1:54], as.numeric) #reconverting back to numeric 
#for the correlation function to work

div_cor &lt;- cor(divorce_num)
corrplot(div_cor, method = &quot;shade&quot;)</code></pre>
<p><img src="/blog/Divorce_files/figure-html/unnamed-chunk-3-1.png" width="1152" /></p>
<p>The correlation plot does provide some interesting insights. Questions 6 and 7 have very little to no correlation with any of the other questions. There are also certain chunks of questions that apparently have a higher correlation with each other than the general average, such as from Question 8 to 30. However, the overall high correlations are not that surprising, as the questions all do have a central theme of marriage and relationships in common, so in some way it is plausible to assume they will all be interlinked with each other in some way. Despite this, some extremely high correlated questions can be removed to get rid of any redundant features.</p>
<pre class="r"><code>findCorrelation(div_cor,
                cutoff = 0.95,
                names = TRUE) #finding questions that have a correlation of 0.95 or higher. </code></pre>
<pre><code>## [1] &quot;Atr19&quot; &quot;Atr36&quot; &quot;Atr20&quot;</code></pre>
<pre class="r"><code>divorce &lt;- divorce[,c(-19, -36)]</code></pre>
<p>Questions 19, 36 and 20 are highly correlated with each other, so 19 and 36 are removed, leaving 20 (a personal choice). To conclude visualizing, the class distribution of the dependent variable can also be plotted.</p>
<pre class="r"><code>#Visualizing class distribution of dependent variable...
x &lt;- as.data.frame(table(divorce[,53]))
names(x)[1] &lt;- &quot;Status&quot;
ggplot(x, aes(Status, Freq, label = Freq, fill = Status)) +
  geom_col() +
  geom_text(nudge_y = 1.5) +
  scale_fill_discrete(name = &quot;Marital Status&quot;,
                      breaks = c(0, 1),
                      labels = c(&quot;Married&quot;,&quot;Divorced&quot;))</code></pre>
<p><img src="/blog/Divorce_files/figure-html/unnamed-chunk-5-1.png" width="864" /></p>
<pre class="r"><code>#Removing temporary dataframes
rm(divorce_num)
rm(x)</code></pre>
<p>As previously mentioned, the number of married and divorced couples answering the questions is almost equally balanced.</p>
</div>
<div id="modelling" class="section level3">
<h3>Modelling</h3>
<p>As this is a simple binary classifaction problem (only two outcomes), the interpretability of the process is not too complicated. The data is partitioned using a 70/30 train-test split, and a 10-fold repeated cross validation resampling strategy is applied. The models used for classification predictions are Naive Bayes (NB), Logistic Regression (Logi), Generalized Linear Model with Regularization Parameters (RGZ) and Support Vector Machines (SVM). To sccuinctly and briefly describe the methods: NB uses probabilistic theory based on Bayes Theorum to classify variables, Logi is a log transformed version of the standard linear regression model making it suitable for classification as opposed to regression, RGZ is version of generalized linear models involving penalty parameters aimed to improve performance when there are numerous feaures involved, and SVM is an advanced technique where features classified on a simulated feature by a specific function utilising linear algebra techniques.</p>
<p>The goal is to find the best model among these four that produces the most accurate predictions. Along with the standard accuracy metric used in the confusion matrices in classification problem, the Receiver Operating Characteristics (ROC) curve will also be utilised, since it provides a more robust validation of the accuracy. It should be noted however, that in cases of binary classifactions, the area under the curve (AUC) for the ROC and the accuracy values are the same. The data is split and the models are run in the following code block.</p>
<pre class="r"><code>#Train-Test Split...
set.seed(1234)
split &lt;- initial_split(divorce, prop = 0.7, strata = &quot;Status&quot;)
train &lt;- training(split)
test &lt;- testing(split)

cv &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10)

#Naive Bayes... (benchmark)
set.seed(1234)
divorce_nbayes &lt;- train(Status ~ .,
                        data = train,
                        method = &quot;nb&quot;,
                        trControl = cv)
#Logistic..
set.seed(1234)
divorce_logi &lt;- train(Status ~ .,
                      data = train,
                      method = &quot;glm&quot;,
                      family = &quot;binomial&quot;,
                      trControl = cv)
#GLM Regularized..
set.seed(1234)
divorce_rgz &lt;- train(Status ~ .,
                     data = train,
                     method = &quot;glmnet&quot;,
                     trControl = cv)
#SVM...
set.seed(1234)
divorce_svm &lt;- train(Status ~ .,
                     data = train,
                     method = &quot;svmRadial&quot;,
                     trControl = cv)</code></pre>
<p>To compare the accuracy of the models on the training set, both the accuracy and the Kappa values are compared. The Kappa value is thought to be more robust as it takes into account the success of the predictions based solely on chance.</p>
<pre class="r"><code>summary(resamples(list(
  N_Bayes = divorce_nbayes,
  Logistic = divorce_logi,
  Regularized = divorce_rgz,
  SVM  = divorce_svm
)))$statistics$Accuracy</code></pre>
<pre><code>##                  Min.   1st Qu. Median      Mean   3rd Qu. Max. NA&#39;s
## N_Bayes     0.4545455 0.6666667   0.75 0.7454196 0.8333333    1    0
## Logistic    0.6666667 0.9166667   1.00 0.9442599 1.0000000    1    0
## Regularized 0.8333333 0.9166667   1.00 0.9659499 1.0000000    1    0
## SVM         0.8333333 0.9166667   1.00 0.9751282 1.0000000    1    0</code></pre>
<pre class="r"><code>summary(resamples(list(
  N_Bayes = divorce_nbayes,
  Logistic = divorce_logi,
  Regularized = divorce_rgz,
  SVM  = divorce_svm
)))$statistics$Kappa</code></pre>
<pre><code>##                  Min.   1st Qu. Median      Mean   3rd Qu. Max. NA&#39;s
## N_Bayes     0.0000000 0.3333333    0.5 0.4976498 0.6666667    1    0
## Logistic    0.3333333 0.8333333    1.0 0.8883405 1.0000000    1    0
## Regularized 0.6666667 0.8333333    1.0 0.9317701 1.0000000    1    0
## SVM         0.6666667 0.8333333    1.0 0.9502008 1.0000000    1    0</code></pre>
<p>The top table highlights the accuracy values, while the bottom table highlights the Kappa values. In both cases, the Naive Bayes (benchmark model) performed the worst, with SVM performing the best. However, the RGZ Model is also very close, which warrants further inspection. It is also highly possible that the SVM due to its complexity may be overfitting. The next step is to analyse the confusion matrices for each of the models. The “positive” class in this case is 1 (divorce).</p>
<pre class="r"><code>nbayes_pred &lt;- predict(divorce_nbayes, test) #Naive Bayes 
confusionMatrix(data = relevel(nbayes_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24 15
##          0  1 10
##                                          
##                Accuracy : 0.68           
##                  95% CI : (0.533, 0.8048)
##     No Information Rate : 0.5            
##     P-Value [Acc &gt; NIR] : 0.007673       
##                                          
##                   Kappa : 0.36           
##                                          
##  Mcnemar&#39;s Test P-Value : 0.001154       
##                                          
##             Sensitivity : 0.9600         
##             Specificity : 0.4000         
##          Pos Pred Value : 0.6154         
##          Neg Pred Value : 0.9091         
##              Prevalence : 0.5000         
##          Detection Rate : 0.4800         
##    Detection Prevalence : 0.7800         
##       Balanced Accuracy : 0.6800         
##                                          
##        &#39;Positive&#39; Class : 1              
## </code></pre>
<pre class="r"><code>logi_pred &lt;- predict(divorce_logi, test) #Logistic
confusionMatrix(data = relevel(logi_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 21  0
##          0  4 25
##                                           
##                Accuracy : 0.92            
##                  95% CI : (0.8077, 0.9778)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 2.231e-10       
##                                           
##                   Kappa : 0.84            
##                                           
##  Mcnemar&#39;s Test P-Value : 0.1336          
##                                           
##             Sensitivity : 0.8400          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.8621          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4200          
##    Detection Prevalence : 0.4200          
##       Balanced Accuracy : 0.9200          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<pre class="r"><code>rgz_pred &lt;- predict(divorce_rgz, test) #Regularized
confusionMatrix(data = relevel(rgz_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24  0
##          0  1 25
##                                           
##                Accuracy : 0.98            
##                  95% CI : (0.8935, 0.9995)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 4.53e-14        
##                                           
##                   Kappa : 0.96            
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9600          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9615          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4800          
##    Detection Prevalence : 0.4800          
##       Balanced Accuracy : 0.9800          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<pre class="r"><code>svm_pred &lt;- predict(divorce_svm, test) #SVM
confusionMatrix(data = relevel(svm_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24  0
##          0  1 25
##                                           
##                Accuracy : 0.98            
##                  95% CI : (0.8935, 0.9995)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 4.53e-14        
##                                           
##                   Kappa : 0.96            
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9600          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9615          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4800          
##    Detection Prevalence : 0.4800          
##       Balanced Accuracy : 0.9800          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<p>Once again, the Naive Bayes model produces the worst results. However, this time the RGZ model and the SVM model have performed exactly the same, with equal accuracy, Kappa and confusion matrix values. This allows for the possibility of actually choosing the RGZ model instead of the SVM, as they are both performing equally, and the RGZ model is preferable as it is much easier to interpret. However, just to be sure, the AUC values for the models can be computed as well.</p>
<pre class="r"><code>pred_ROCR_nbayes &lt;- prediction(as.numeric(nbayes_pred), 
                               as.numeric(test$Status))

perf_ROCR_nbayes &lt;- performance(pred_ROCR_nbayes, 
                                measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_nbayes &lt;- performance(pred_ROCR_nbayes, measure = &quot;auc&quot;)
auc_ROCR_nbayes &lt;- auc_ROCR_nbayes@y.values[[1]]

pred_ROCR_logi &lt;- prediction(as.numeric(logi_pred), 
                             as.numeric(test$Status))

perf_ROCR_logi &lt;- performance(pred_ROCR_logi, 
                              measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_logi &lt;- performance(pred_ROCR_logi, measure = &quot;auc&quot;)
auc_ROCR_logi &lt;- auc_ROCR_logi@y.values[[1]]

pred_ROCR_rgz &lt;- prediction(as.numeric(rgz_pred), 
                             as.numeric(test$Status))

perf_ROCR_rgz &lt;- performance(pred_ROCR_rgz, 
                              measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_rgz &lt;- performance(pred_ROCR_rgz, measure = &quot;auc&quot;)
auc_ROCR_rgz &lt;- auc_ROCR_rgz@y.values[[1]]

pred_ROCR_svm &lt;- prediction(as.numeric(svm_pred), 
                            as.numeric(test$Status))

perf_ROCR_svm &lt;- performance(pred_ROCR_svm, 
                             measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_svm &lt;- performance(pred_ROCR_svm, measure = &quot;auc&quot;)
auc_ROCR_svm &lt;- auc_ROCR_svm@y.values[[1]]

data.frame(&quot;Naive Bayes&quot; = auc_ROCR_nbayes,&quot;Logistic&quot; = auc_ROCR_logi, &quot;RGZ&quot; = auc_ROCR_rgz, &quot;SVM&quot; = auc_ROCR_svm) </code></pre>
<pre><code>##   Naive.Bayes Logistic  RGZ  SVM
## 1        0.68     0.92 0.98 0.98</code></pre>
<p>As expected, the accuracy and the AUC values for the RGZ and SVM models are the same. As both the RGZ and SVM are performing equally well, the RGZ model is preferred to be the final model. Just for a visual, the ROC curves for Naive Bayes, Logistic and RGZ models are plotted below. The curve for RGZ resembles an almost perfect ROC curve, which is backed up by the exceptionally high accuracy.</p>
<pre class="r"><code>plot(perf_ROCR_nbayes, col = &quot;black&quot;, lty = 1)
plot(perf_ROCR_logi, add = TRUE, col = &quot;red&quot;, lty = 2)
plot(perf_ROCR_rgz, add = TRUE, col = &quot;blue&quot;, lty = 3)
legend(0.8, 0.2, legend = c(&quot;Naive Bayes&quot;, &quot;Logistic&quot;, &quot;RGZ&quot;),
       col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(1, 2, 3), cex = 1)</code></pre>
<p><img src="/blog/Divorce_files/figure-html/unnamed-chunk-10-1.png" width="864" /></p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>Since the Generalized Linear Models with Regularization Parameters is being used, the actual parameter values should be noted as well.</p>
<pre class="r"><code>divorce_rgz</code></pre>
<pre><code>## glmnet 
## 
## 120 samples
##  52 predictor
##   2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 107, 109, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda      Accuracy   Kappa    
##   0.10   0.02517439  0.9659499  0.9317701
##   0.10   0.07960842  0.9626049  0.9249679
##   0.10   0.25174392  0.9601049  0.9199679
##   0.55   0.02517439  0.9474534  0.8944101
##   0.55   0.07960842  0.9424534  0.8845724
##   0.55   0.25174392  0.9150466  0.8291400
##   1.00   0.02517439  0.9373776  0.8744358
##   1.00   0.07960842  0.9348776  0.8693976
##   1.00   0.25174392  0.8833566  0.7654431
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda
##  = 0.02517439.</code></pre>
<p>The model maximized accuracy with an alpha value of 0.1 and lambda value of 0.025. Having found the exact model parameters, an easy to interpret model has been developed for this specific use case which provides exceptionally accurate results.</p>
</div>


              
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="/blog/strokes/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">Classifying Strokes - Balancing Classes with SMOTE</span>
    </a>
  

  
    <a class="pagination__item" href="/blog/brent-oil/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Brent Oil Prices - Time Series with ML Models</a>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
  
     
    
  
     
    
  
     
    
  
     
    
  
     
    
  
     
    
  
     
    
     
</div>

            <p>© 2021 fazi_rda</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.49e4d8a384357d9b445b87371863419937ede9fa77737522ffb633073aebfa44.js" integrity="sha256-SeTYo4Q1fZtEW4c3GGNBmTft6fp3c3Ui/7YzBzrr&#43;kQ=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
